{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from sys import path\n",
    "from os import getcwd\n",
    "from os.path import join, dirname, abspath, pardir\n",
    "\n",
    "# Add the `src` directory to the path.\n",
    "parent_dir = abspath(join(getcwd(), pardir))\n",
    "src_dir = join(parent_dir, 'src')\n",
    "path.append(src_dir)\n",
    "\n",
    "# Import the `train` module.\n",
    "from train import train_sac\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import dm_control\n",
    "from dm_control.rl import control\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dm_control.suite.load('manipulator', 'insert_ball', visualize_reward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dimension value must be integer or None or have an __index__ method, got value ''arm_pos'' with type '<class 'str'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mInput(env\u001b[39m.\u001b[39;49mobservation_spec())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/reinforcement_learning/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/reinforcement_learning/lib/python3.10/site-packages/tensorflow/python/framework/tensor_shape.py:214\u001b[0m, in \u001b[0;36mDimension.__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(value\u001b[39m.\u001b[39m\u001b[39m__index__\u001b[39m())\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mDimension value must be integer or None or have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39man __index__ method, got value \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{0!r}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m with type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{1!r}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    217\u001b[0m           value, \u001b[39mtype\u001b[39m(value))) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    219\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDimension \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m must be >= 0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value)\n",
      "\u001b[0;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got value ''arm_pos'' with type '<class 'str'>'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_sac()\n",
      "File \u001b[0;32m~/Masters/CS/Machine-Learning/Reinforcement-Learning-CS/src/train.py:25\u001b[0m, in \u001b[0;36mtrain_sac\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m env \u001b[39m=\u001b[39m dm_control\u001b[39m.\u001b[39msuite\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmanipulator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minsert_ball\u001b[39m\u001b[39m'\u001b[39m, visualize_reward\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Create the actor and critic models.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m actor \u001b[39m=\u001b[39m actor_model(env\u001b[39m=\u001b[39;49menv)\n\u001b[1;32m     26\u001b[0m critic \u001b[39m=\u001b[39m critic_model(env\u001b[39m=\u001b[39menv)\n\u001b[1;32m     28\u001b[0m \u001b[39m# Create the SAC agent.\u001b[39;00m\n",
      "File \u001b[0;32m~/Masters/CS/Machine-Learning/Reinforcement-Learning-CS/src/utils.py:14\u001b[0m, in \u001b[0;36mactor_model\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mactor_model\u001b[39m(env: control\u001b[39m.\u001b[39mEnvironment):\n\u001b[0;32m---> 14\u001b[0m     inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(env\u001b[39m.\u001b[39;49mobservation_spec()\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m],))\n\u001b[1;32m     15\u001b[0m     x \u001b[39m=\u001b[39m Dense(\u001b[39m32\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(inputs)\n\u001b[1;32m     16\u001b[0m     x \u001b[39m=\u001b[39m Dense(\u001b[39m32\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_sac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b329387e251b95764b8f65684563519503b45dc8027da482b0a7bdbaa4a30d3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
