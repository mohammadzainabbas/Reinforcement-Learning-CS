{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCOanHc8JH_"
      },
      "source": [
        "## Demo for step-by-step training with PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sOmCoOrF0F8"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import functools\n",
        "import os\n",
        "from os import getcwd\n",
        "from os.path import join\n",
        "from IPython.display import HTML, clear_output\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "from brax import envs\n",
        "from brax import jumpy as jp\n",
        "from brax.io import html\n",
        "from brax.io import model\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm8zbPBcJ5RJ"
      },
      "source": [
        "#### Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "NaJDZqhCLovU",
        "outputId": "50994b20-d788-4264-af00-a3f06d58f943"
      },
      "outputs": [],
      "source": [
        "env_name = \"grasp\"\n",
        "env = envs.get_environment(env_name=env_name)\n",
        "state = env.reset(rng=jp.random_prngkey(seed=0))\n",
        "\n",
        "HTML(html.render(env.sys, [state.qp]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_ppo(num_timesteps):\n",
        "\tprint(f\"Training PPO for '{num_timesteps}' timesteps'\")\n",
        "\n",
        "\ttrain_fn = functools.partial(ppo.train, num_timesteps=num_timesteps, num_evals=10, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=20, num_minibatches=32, num_updates_per_batch=2, discounting=0.99, learning_rate=3e-4, entropy_cost=0.001, num_envs=2048, batch_size=256)\n",
        "\n",
        "\tmax_y = 100\n",
        "\tmin_y = 0\n",
        "\n",
        "\txdata, ydata = [], []\n",
        "\ttimes = [datetime.now()]\n",
        "\n",
        "\tdef progress(num_steps, metrics):\n",
        "\t\ttimes.append(datetime.now())\n",
        "\t\txdata.append(num_steps)\n",
        "\t\tydata.append(metrics['eval/episode_reward'])\n",
        "\t\tclear_output(wait=True)\n",
        "\t\t# plt.xlim([0, train_fn.keywords['num_timesteps']])\n",
        "\t\t# plt.ylim([min_y, max_y])\n",
        "\t\t# plt.xlabel('# environment steps')\n",
        "\t\t# plt.ylabel('reward per episode')\n",
        "\t\t# plt.plot(xdata, ydata)\n",
        "\t\t# plt.show()\n",
        "\n",
        "\tmake_inference_fn, params, _ = train_fn(environment=env, progress_fn=progress)\n",
        "\tprint(f'time to jit: {times[1] - times[0]}')\n",
        "\tprint(f'time to train: {times[-1] - times[1]}')\n",
        "\n",
        "\treturn make_inference_fn, params, times, xdata, ydata\n",
        "\n",
        "def visual_rollout(inference_fn, env_name, steps=100, seed=0):\n",
        "\tenv = envs.create(env_name=env_name)\n",
        "\tjit_env_reset = jax.jit(env.reset)\n",
        "\tjit_env_step = jax.jit(env.step)\n",
        "\tjit_inference_fn = jax.jit(inference_fn)\n",
        "\n",
        "\trollout = []\n",
        "\trng = jax.random.PRNGKey(seed=seed)\n",
        "\tstate = jit_env_reset(rng=rng)\n",
        "\tfor _ in range(steps):\n",
        "\t\trollout.append(state)\n",
        "\t\tact_rng, rng = jax.random.split(rng)\n",
        "\t\tact, _ = jit_inference_fn(state.obs, act_rng)\n",
        "\t\tstate = jit_env_step(state, act)\n",
        "\n",
        "\treturn env.sys, [s.qp for s in rollout]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_num_timesteps = [1_000, 1_000_000, 100_000_000]\n",
        "vis_steps = [100, 100, 100]\n",
        "\n",
        "for num_timesteps, idx in enumerate(training_num_timesteps):\n",
        "\tmake_inference_fn, params, times, xdata, ydata = train_ppo(num_timesteps)\n",
        "\tinference_fn = make_inference_fn(params)\n",
        "\tsys, rollout = visual_rollout(inference_fn, env_name, steps=100, seed=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "25iky4mITUhh"
      },
      "source": [
        "# Training\n",
        "\n",
        "Brax provides out of the box the following training algorithms:\n",
        "\n",
        "* [Proximal policy optimization](https://github.com/google/brax/blob/main/brax/training/agents/ppo/train.py)\n",
        "\n",
        "Trainers take as input an environment function and some hyperparameters, and return an inference function to operate the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "4vgMSWODfyMC",
        "outputId": "44c83693-a890-453c-b9a3-5eebaa69760a"
      },
      "outputs": [],
      "source": [
        "train_fn = functools.partial(ppo.train, num_timesteps=600_000_000, num_evals=10, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=20, num_minibatches=32, num_updates_per_batch=2, discounting=0.99, learning_rate=3e-4, entropy_cost=0.001, num_envs=2048, batch_size=256)\n",
        "\n",
        "max_y = 100\n",
        "min_y = 0\n",
        "\n",
        "xdata, ydata = [], []\n",
        "times = [datetime.now()]\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  xdata.append(num_steps)\n",
        "  ydata.append(metrics['eval/episode_reward'])\n",
        "  clear_output(wait=True)\n",
        "  plt.xlim([0, train_fn.keywords['num_timesteps']])\n",
        "  plt.ylim([min_y, max_y])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.plot(xdata, ydata)\n",
        "  plt.show()\n",
        "\n",
        "make_inference_fn, params, _ = train_fn(environment=env, progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FGPyeF8Jsj_M"
      },
      "source": [
        "#### Saving and Loading Policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgB52sgjDhvi"
      },
      "outputs": [],
      "source": [
        "output_model_path = join(getcwd(), \"ppo_params\")\n",
        "model.save_params(output_model_path, params)\n",
        "params = model.load_params(output_model_path)\n",
        "inference_fn = make_inference_fn(params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uUs9hbUKIH71"
      },
      "source": [
        "### Visualizing a Policy's Behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "RNMLEyaTspEM",
        "outputId": "c03b0854-32a9-4286-9c3f-26f2f158e617"
      },
      "outputs": [],
      "source": [
        "env = envs.create(env_name=env_name)\n",
        "jit_env_reset = jax.jit(env.reset)\n",
        "jit_env_step = jax.jit(env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "\n",
        "rollout = []\n",
        "rng = jax.random.PRNGKey(seed=0)\n",
        "state = jit_env_reset(rng=rng)\n",
        "for _ in range(1000):\n",
        "  rollout.append(state)\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "  act, _ = jit_inference_fn(state.obs, act_rng)\n",
        "  state = jit_env_step(state, act)\n",
        "\n",
        "HTML(html.render(env.sys, [s.qp for s in rollout]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Brax Training.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "reinforcement_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b329387e251b95764b8f65684563519503b45dc8027da482b0a7bdbaa4a30d3e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
