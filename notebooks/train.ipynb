{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/bdmagr1/abbas/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/glfw/__init__.py:912: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "/usr/users/bdmagr1/abbas/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium/envs/registration.py:313: UserWarning: \u001b[33mWARN: No module named 'imageio'\u001b[0m\n",
      "  logger.warn(str(e))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "  import brax\n",
    "except ImportError:\n",
    "  !pip install git+https://github.com/google/brax.git@main\n",
    "  clear_output()\n",
    "  import brax\n",
    "\n",
    "from brax import envs\n",
    "from brax import jumpy as jp\n",
    "from brax.io import html\n",
    "from brax.io import model\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.sac import train as sac\n",
    "\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m'\u001b[39;49m\u001b[39mFetchSlide-v2\u001b[39;49m\u001b[39m'\u001b[39;49m, max_episode_steps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium/envs/registration.py:641\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m     render_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs)\n\u001b[1;32m    642\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    643\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    644\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    646\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium_robotics/envs/fetch/slide.py:163\u001b[0m, in \u001b[0;36mMujocoFetchSlideEnv.__init__\u001b[0;34m(self, reward_type, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, reward_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    157\u001b[0m     initial_qpos \u001b[39m=\u001b[39m {\n\u001b[1;32m    158\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrobot0:slide0\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.05\u001b[39m,\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrobot0:slide1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.48\u001b[39m,\n\u001b[1;32m    160\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrobot0:slide2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.0\u001b[39m,\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobject0:joint\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m1.7\u001b[39m, \u001b[39m1.1\u001b[39m, \u001b[39m0.41\u001b[39m, \u001b[39m1.0\u001b[39m, \u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m],\n\u001b[1;32m    162\u001b[0m     }\n\u001b[0;32m--> 163\u001b[0m     MujocoFetchEnv\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    165\u001b[0m         model_path\u001b[39m=\u001b[39;49mMODEL_XML_PATH,\n\u001b[1;32m    166\u001b[0m         has_object\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    167\u001b[0m         block_gripper\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    168\u001b[0m         n_substeps\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m    169\u001b[0m         gripper_extra_height\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m0.02\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m         target_in_the_air\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    171\u001b[0m         target_offset\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49marray([\u001b[39m0.4\u001b[39;49m, \u001b[39m0.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m]),\n\u001b[1;32m    172\u001b[0m         obj_range\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m         target_range\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m,\n\u001b[1;32m    174\u001b[0m         distance_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m,\n\u001b[1;32m    175\u001b[0m         initial_qpos\u001b[39m=\u001b[39;49minitial_qpos,\n\u001b[1;32m    176\u001b[0m         reward_type\u001b[39m=\u001b[39;49mreward_type,\n\u001b[1;32m    177\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    179\u001b[0m     EzPickle\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, reward_type\u001b[39m=\u001b[39mreward_type, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium_robotics/envs/fetch/fetch_env.py:293\u001b[0m, in \u001b[0;36mMujocoFetchEnv.__init__\u001b[0;34m(self, default_camera_config, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, default_camera_config: \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m DEFAULT_CAMERA_CONFIG, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 293\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(default_camera_config\u001b[39m=\u001b[39;49mdefault_camera_config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium_robotics/envs/fetch/fetch_env.py:69\u001b[0m, in \u001b[0;36mget_base_fetch_env.<locals>.BaseFetchEnv.__init__\u001b[0;34m(self, gripper_extra_height, block_gripper, has_object, target_in_the_air, target_offset, obj_range, target_range, distance_threshold, reward_type, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistance_threshold \u001b[39m=\u001b[39m distance_threshold\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_type \u001b[39m=\u001b[39m reward_type\n\u001b[0;32m---> 69\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(n_actions\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium_robotics/envs/robot_env.py:282\u001b[0m, in \u001b[0;36mMujocoRobotEnv.__init__\u001b[0;34m(self, default_camera_config, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_utils \u001b[39m=\u001b[39m mujoco_utils\n\u001b[1;32m    280\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 282\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco_rendering\u001b[39;00m \u001b[39mimport\u001b[39;00m MujocoRenderer\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmujoco_renderer \u001b[39m=\u001b[39m MujocoRenderer(\n\u001b[1;32m    285\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, default_camera_config\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglfw\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimageio\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmujoco\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "env = gym.make('FetchSlide-v2', max_episode_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b329387e251b95764b8f65684563519503b45dc8027da482b0a7bdbaa4a30d3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
