{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "  import brax\n",
    "except ImportError:\n",
    "  !pip install git+https://github.com/google/brax.git@main\n",
    "  clear_output()\n",
    "  import brax\n",
    "\n",
    "import torch\n",
    "from brax import envs\n",
    "from brax.envs import env as brax_env\n",
    "from brax.envs.to_torch import JaxToTorchWrapper\n",
    "from brax import jumpy as jp\n",
    "from brax.io import html\n",
    "from brax.io import model\n",
    "\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.sac import train as sac\n",
    "\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchSlide-v2', max_episode_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env = JaxToTorchWrapper(env, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<JaxToTorchWrapper<TimeLimit<OrderEnforcing<PassiveEnvChecker<MujocoFetchSlideEnv<FetchSlide-v2>>>>>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = gym_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'observation': array([ 9.96056349e-01,  7.48908387e-01,  4.12666572e-01,  9.04922569e-01,\n",
       "          6.83324140e-01,  4.14022562e-01, -9.11337793e-02, -6.55842470e-02,\n",
       "          1.35598971e-03, -2.03562423e-06,  1.47158977e-03, -5.32964269e-03,\n",
       "          1.24548654e-04, -1.88202238e-02,  1.22698261e-03, -5.56736934e-05,\n",
       "          6.43864204e-05,  1.86450993e-02, -4.64845340e-04, -4.83036131e-03,\n",
       "         -1.26969656e-03,  5.03838696e-05,  7.97511346e-06,  4.66194862e-07,\n",
       "          5.49241789e-05]),\n",
       "  'achieved_goal': array([0.90492257, 0.68332414, 0.41402256]),\n",
       "  'desired_goal': array([1.67190182, 0.88547116, 0.41401894])},\n",
       " {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.rand(gym_env.action_space.shape, device=DEVICE) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8037, -0.1575, -0.6071,  0.4965])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gym_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/brax/envs/to_torch.py:59\u001b[0m, in \u001b[0;36mJaxToTorchWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     58\u001b[0m   action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction(action)\n\u001b[0;32m---> 59\u001b[0m   obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     60\u001b[0m   obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(obs)\n\u001b[1;32m     61\u001b[0m   reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward(reward)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "gym_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m obs, reward, done, info \u001b[39m=\u001b[39m gym_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/brax/envs/to_torch.py:59\u001b[0m, in \u001b[0;36mJaxToTorchWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     58\u001b[0m   action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction(action)\n\u001b[0;32m---> 59\u001b[0m   obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     60\u001b[0m   obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(obs)\n\u001b[1;32m     61\u001b[0m   reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward(reward)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "obs, reward, done, info = gym_env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b329387e251b95764b8f65684563519503b45dc8027da482b0a7bdbaa4a30d3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
