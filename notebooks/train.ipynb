{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/bdmagr1/abbas/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/users/bdmagr1/abbas/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/glfw/__init__.py:912: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "#@title Import Brax and some helper modules\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from typing import Any, Callable, Dict, Optional, Sequence\n",
    "\n",
    "try:\n",
    "\timport brax\n",
    "except ImportError:\n",
    "\t!pip install git+https://github.com/google/brax.git@main\n",
    "\tclear_output()\n",
    "\timport brax\n",
    "\n",
    "from brax import envs\n",
    "from brax.envs import to_torch\n",
    "from brax.io import metrics\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# have torch allocate on device first, to prevent JAX from swallowing up all the\n",
    "# GPU memory. By default JAX will pre-allocate 90% of the available GPU memory:\n",
    "# https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
    "DEVICE = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "v = torch.ones(1, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_name = f'FetchSlide-v2'\n",
    "if gym_name not in gym.envs.registry.keys():\n",
    "\tentry_point = functools.partial(envs.create_gym_env, env_name=gym_name)\n",
    "\tgym.register(gym_name, entry_point=entry_point)\n",
    "# env = gym.make(gym_name, batch_size=num_envs, episode_length=episode_length)\n",
    "env = gym.make(gym_name)\n",
    "# automatically convert between jax ndarrays and torch tensors:\n",
    "env = to_torch.JaxToTorchWrapper(env, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<JaxToTorchWrapper<TimeLimit<OrderEnforcing<PassiveEnvChecker<MujocoFetchSlideEnv<FetchSlide-v2>>>>>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.zeros(env.action_space.shape).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ({'observation': [ 9.95910465e-01,  7.48913814e-01,  4.16069922e-01,  1.04896346e+00,\n",
    "        8.36844884e-01,  4.14023388e-01,  5.30529948e-02,  8.79310709e-02,\n",
    "       -2.04653474e-03,  0.00000000e+00,  0.00000000e+00,  5.42714762e-03,\n",
    "       -6.44712672e-04, -2.40447348e-02, -5.85025884e-05,  3.47088392e-05,\n",
    "       -2.81409006e-03, -1.87731081e-02, -1.77318698e-03, -6.81228636e-03,\n",
    "        1.57984089e-05, -1.63635264e-06,  2.89131317e-03,  6.38642398e-07,\n",
    "        1.41205628e-07], 'achieved_goal': [1.04896346, 0.83684488, 0.41402339], 'desired_goal': [1.60595681, 0.46561395, 0.41401894]}, -1.0, False, False, {'is_success': 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "({'observation': array([ 9.95910465e-01,  7.48913814e-01,  4.16069922e-01,  1.04896346e+00,\n",
      "        8.36844884e-01,  4.14023388e-01,  5.30529948e-02,  8.79310709e-02,\n",
      "       -2.04653474e-03,  0.00000000e+00,  0.00000000e+00,  5.42714762e-03,\n",
      "       -6.44712672e-04, -2.40447348e-02, -5.85025884e-05,  3.47088392e-05,\n",
      "       -2.81409006e-03, -1.87731081e-02, -1.77318698e-03, -6.81228636e-03,\n",
      "        1.57984089e-05, -1.63635264e-06,  2.89131317e-03,  6.38642398e-07,\n",
      "        1.41205628e-07]), 'achieved_goal': array([1.04896346, 0.83684488, 0.41402339]), 'desired_goal': array([1.60595681, 0.46561395, 0.41401894])}, -1.0, False, False, {'is_success': 0.0})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/brax/envs/to_torch.py:61\u001b[0m, in \u001b[0;36mJaxToTorchWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mprint\u001b[39m(action)\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mstep(action))\n\u001b[0;32m---> 61\u001b[0m obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     62\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(obs)\n\u001b[1;32m     63\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward(reward)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env = JaxToTorchWrapper(env, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = gym_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.rand(gym_env.action_space.shape, device=DEVICE) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(gym.envs.registry.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, info = gym_env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b329387e251b95764b8f65684563519503b45dc8027da482b0a7bdbaa4a30d3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
