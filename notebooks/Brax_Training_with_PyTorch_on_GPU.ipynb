{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trVNqxHmGISS"
      },
      "source": [
        "# Training in Brax with PyTorch on GPUs\n",
        "\n",
        "Brax is ready to integrate into other research toolkits by way of the [OpenAI Gym](https://gym.openai.com/) interface.  Brax environments convert to Gym environments using either [GymWrapper](https://github.com/google/brax/blob/main/brax/envs/wrappers.py) for single environments, or [VectorGymWrapper](https://github.com/google/brax/blob/main/brax/envs/wrappers.py) for batched (parallelized) environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GJhPpM5ZPrpq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/users/bdmagr1/abbas/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/glfw/__init__.py:912: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
            "  warnings.warn(message, GLFWError)\n"
          ]
        }
      ],
      "source": [
        "#@title Import Brax and some helper modules\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import math\n",
        "import time\n",
        "from typing import Any, Callable, Dict, Optional, Sequence\n",
        "\n",
        "try:\n",
        "\timport brax\n",
        "except ImportError:\n",
        "\t!pip install git+https://github.com/google/brax.git@main\n",
        "\tclear_output()\n",
        "\timport brax\n",
        "\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import metrics\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# have torch allocate on device first, to prevent JAX from swallowing up all the\n",
        "# GPU memory. By default JAX will pre-allocate 90% of the available GPU memory:\n",
        "# https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
        "DEVICE = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
        "v = torch.ones(1, device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFCkfu8Qwre"
      },
      "source": [
        "Here is a PPO Agent written in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fWJE4b5BHeH7"
      },
      "outputs": [],
      "source": [
        "class PPOAgent(nn.Module):\n",
        "\t\"\"\"\n",
        "\tStandard PPO Agent with GAE and observation normalization.\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, policy_layers: Sequence[int], value_layers: Sequence[int], entropy_cost: float, discounting: float, reward_scaling: float, device: str):\n",
        "\t\tsuper(PPOAgent, self).__init__()\n",
        "\t\tpolicy = []\n",
        "\n",
        "\t\tfor w1, w2 in zip(policy_layers, policy_layers[1:]):\n",
        "\t\t\tpolicy.append(nn.Linear(w1, w2))\n",
        "\t\t\tpolicy.append(nn.SiLU())\n",
        "\t\tpolicy.pop()  # drop the final activation\n",
        "\t\tself.policy = nn.Sequential(*policy)\n",
        "\n",
        "\t\tvalue = []\n",
        "\t\tfor w1, w2 in zip(value_layers, value_layers[1:]):\n",
        "\t\t\tvalue.append(nn.Linear(w1, w2))\n",
        "\t\t\tvalue.append(nn.SiLU())\n",
        "\t\tvalue.pop()  # drop the final activation\n",
        "\t\tself.value = nn.Sequential(*value)\n",
        "\n",
        "\t\tself.num_steps = torch.zeros((), device=device)\n",
        "\t\tself.running_mean = torch.zeros(policy_layers[0], device=device)\n",
        "\t\tself.running_variance = torch.zeros(policy_layers[0], device=device)\n",
        "\n",
        "\t\tself.entropy_cost = entropy_cost\n",
        "\t\tself.discounting = discounting\n",
        "\t\tself.reward_scaling = reward_scaling\n",
        "\t\tself.lambda_ = 0.95\n",
        "\t\tself.epsilon = 0.3\n",
        "\t\tself.device = device\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef dist_create(self, logits):\n",
        "\t\t\"\"\"\n",
        "\t\tNormal followed by tanh.\n",
        "\n",
        "\t\ttorch.distribution doesn't work with torch.jit, so we roll our own.\n",
        "\t\t\"\"\"\n",
        "\t\tloc, scale = torch.split(logits, logits.shape[-1] // 2, dim=-1)\n",
        "\t\tscale = F.softplus(scale) + .001\n",
        "\t\treturn loc, scale\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef dist_sample_no_postprocess(self, loc, scale):\n",
        "\t\treturn torch.normal(loc, scale)\n",
        "\n",
        "\t@classmethod\n",
        "\tdef dist_postprocess(cls, x):\n",
        "\t\treturn torch.tanh(x)\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef dist_entropy(self, loc, scale):\n",
        "\t\tlog_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "\t\tentropy = 0.5 + log_normalized\n",
        "\t\tentropy = entropy * torch.ones_like(loc)\n",
        "\t\tdist = torch.normal(loc, scale)\n",
        "\t\tlog_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
        "\t\tentropy = entropy + log_det_jacobian\n",
        "\t\treturn entropy.sum(dim=-1)\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef dist_log_prob(self, loc, scale, dist):\n",
        "\t\tlog_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
        "\t\tlog_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "\t\tlog_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
        "\t\tlog_prob = log_unnormalized - log_normalized - log_det_jacobian\n",
        "\t\treturn log_prob.sum(dim=-1)\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef update_normalization(self, observation):\n",
        "\t\tself.num_steps += observation.shape[0] * observation.shape[1]\n",
        "\t\tinput_to_old_mean = observation - self.running_mean\n",
        "\t\tmean_diff = torch.sum(input_to_old_mean / self.num_steps, dim=(0, 1))\n",
        "\t\tself.running_mean = self.running_mean + mean_diff\n",
        "\t\tinput_to_new_mean = observation - self.running_mean\n",
        "\t\tvar_diff = torch.sum(input_to_new_mean * input_to_old_mean, dim=(0, 1))\n",
        "\t\tself.running_variance = self.running_variance + var_diff\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef normalize(self, observation):\n",
        "\t\tvariance = self.running_variance / (self.num_steps + 1.0)\n",
        "\t\tvariance = torch.clip(variance, 1e-6, 1e6)\n",
        "\t\treturn ((observation - self.running_mean) / variance.sqrt()).clip(-5, 5)\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef get_logits_action(self, observation):\n",
        "\t\tobservation = self.normalize(observation)\n",
        "\t\tlogits = self.policy(observation)\n",
        "\t\tloc, scale = self.dist_create(logits)\n",
        "\t\taction = self.dist_sample_no_postprocess(loc, scale)\n",
        "\t\treturn logits, action\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef compute_gae(self, truncation, termination, reward, values,\n",
        "\t\t\t\t\tbootstrap_value):\n",
        "\t\ttruncation_mask = 1 - truncation\n",
        "\t\t# Append bootstrapped value to get [v1, ..., v_t+1]\n",
        "\t\tvalues_t_plus_1 = torch.cat(\n",
        "\t\t\t[values[1:], torch.unsqueeze(bootstrap_value, 0)], dim=0)\n",
        "\t\tdeltas = reward + self.discounting * (\n",
        "\t\t\t1 - termination) * values_t_plus_1 - values\n",
        "\t\tdeltas *= truncation_mask\n",
        "\n",
        "\t\tacc = torch.zeros_like(bootstrap_value)\n",
        "\t\tvs_minus_v_xs = torch.zeros_like(truncation_mask)\n",
        "\n",
        "\t\tfor ti in range(truncation_mask.shape[0]):\n",
        "\t\t\tti = truncation_mask.shape[0] - ti - 1\n",
        "\t\t\tacc = deltas[ti] + self.discounting * (\n",
        "\t\t\t\t1 - termination[ti]) * truncation_mask[ti] * self.lambda_ * acc\n",
        "\t\t\tvs_minus_v_xs[ti] = acc\n",
        "\n",
        "\t\t# Add V(x_s) to get v_s.\n",
        "\t\tvs = vs_minus_v_xs + values\n",
        "\t\tvs_t_plus_1 = torch.cat([vs[1:], torch.unsqueeze(bootstrap_value, 0)], 0)\n",
        "\t\tadvantages = (reward + self.discounting *\n",
        "\t\t\t\t\t(1 - termination) * vs_t_plus_1 - values) * truncation_mask\n",
        "\t\treturn vs, advantages\n",
        "\n",
        "\t@torch.jit.export\n",
        "\tdef loss(self, td: Dict[str, torch.Tensor]):\n",
        "\t\tobservation = self.normalize(td['observation'])\n",
        "\t\tpolicy_logits = self.policy(observation[:-1])\n",
        "\t\tbaseline = self.value(observation)\n",
        "\t\tbaseline = torch.squeeze(baseline, dim=-1)\n",
        "\n",
        "\t\t# Use last baseline value (from the value function) to bootstrap.\n",
        "\t\tbootstrap_value = baseline[-1]\n",
        "\t\tbaseline = baseline[:-1]\n",
        "\t\treward = td['reward'] * self.reward_scaling\n",
        "\t\ttermination = td['done'] * (1 - td['truncation'])\n",
        "\n",
        "\t\tloc, scale = self.dist_create(td['logits'])\n",
        "\t\tbehaviour_action_log_probs = self.dist_log_prob(loc, scale, td['action'])\n",
        "\t\tloc, scale = self.dist_create(policy_logits)\n",
        "\t\ttarget_action_log_probs = self.dist_log_prob(loc, scale, td['action'])\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tvs, advantages = self.compute_gae(\n",
        "\t\t\t\ttruncation=td['truncation'],\n",
        "\t\t\t\ttermination=termination,\n",
        "\t\t\t\treward=reward,\n",
        "\t\t\t\tvalues=baseline,\n",
        "\t\t\t\tbootstrap_value=bootstrap_value)\n",
        "\t\trho_s = torch.exp(target_action_log_probs - behaviour_action_log_probs)\n",
        "\t\tsurrogate_loss1 = rho_s * advantages\n",
        "\t\tsurrogate_loss2 = rho_s.clip(1 - self.epsilon,\n",
        "\t\t\t\t\t\t\t\t\t1 + self.epsilon) * advantages\n",
        "\t\tpolicy_loss = -torch.mean(torch.minimum(surrogate_loss1, surrogate_loss2))\n",
        "\n",
        "\t\t# Value function loss\n",
        "\t\tv_error = vs - baseline\n",
        "\t\tv_loss = torch.mean(v_error * v_error) * 0.5 * 0.5\n",
        "\n",
        "\t\t# Entropy reward\n",
        "\t\tentropy = torch.mean(self.dist_entropy(loc, scale))\n",
        "\t\tentropy_loss = self.entropy_cost * -entropy\n",
        "\n",
        "\t\treturn policy_loss + v_loss + entropy_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWbuk7IAR0SU"
      },
      "source": [
        "Finally, some code for unrolling and batching environment data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "D3y5o7-oSBm-"
      },
      "outputs": [],
      "source": [
        "StepData = collections.namedtuple(\n",
        "\t'StepData',\n",
        "\t('observation', 'logits', 'action', 'reward', 'done', 'truncation'))\n",
        "\n",
        "def sd_map(f: Callable[..., torch.Tensor], *sds) -> StepData:\n",
        "\t\"\"\"Map a function over each field in StepData.\"\"\"\n",
        "\titems = {}\n",
        "\tkeys = sds[0]._asdict().keys()\n",
        "\tfor k in keys:\n",
        "\t\titems[k] = f(*[sd._asdict()[k] for sd in sds])\n",
        "\treturn StepData(**items)\n",
        "\n",
        "def eval_unroll(agent, env, length):\n",
        "\t\"\"\"Return number of episodes and average reward for a single unroll.\"\"\"\n",
        "\tobservation = env.reset()\n",
        "\tepisodes = torch.zeros((), device=agent.device)\n",
        "\tepisode_reward = torch.zeros((), device=agent.device)\n",
        "\tfor _ in range(length):\n",
        "\t\t_, action = agent.get_logits_action(observation)\n",
        "\t\tobservation, reward, done, _ = env.step(PPOAgent.dist_postprocess(action))\n",
        "\t\tepisodes += torch.sum(done)\n",
        "\t\tepisode_reward += torch.sum(reward)\n",
        "\treturn episodes, episode_reward / episodes\n",
        "\n",
        "def train_unroll(agent, env, observation, num_unrolls, unroll_length):\n",
        "\t\"\"\"Return step data over multple unrolls.\"\"\"\n",
        "\tsd = StepData([], [], [], [], [], [])\n",
        "\tfor _ in range(num_unrolls):\n",
        "\t\tone_unroll = StepData([observation], [], [], [], [], [])\n",
        "\t\tfor _ in range(unroll_length):\n",
        "\t\t\tlogits, action = agent.get_logits_action(observation)\n",
        "\t\t\tobservation, reward, done, info = env.step(PPOAgent.dist_postprocess(action))\n",
        "\t\t\tone_unroll.observation.append(observation)\n",
        "\t\t\tone_unroll.logits.append(logits)\n",
        "\t\t\tone_unroll.action.append(action)\n",
        "\t\t\tone_unroll.reward.append(reward)\n",
        "\t\t\tone_unroll.done.append(done)\n",
        "\t\t\tone_unroll.truncation.append(info['truncation'])\n",
        "\t\tone_unroll = sd_map(torch.stack, one_unroll)\n",
        "\t\tsd = sd_map(lambda x, y: x + [y], sd, one_unroll)\n",
        "\ttd = sd_map(torch.stack, sd)\n",
        "\treturn observation, td\n",
        "\n",
        "def train(\n",
        "\t\t# env_name: str = 'ant',\n",
        "\t\tenv_name: str = 'FetchSlide-v2',\n",
        "\t\tnum_envs: int = 2048,\n",
        "\t\tepisode_length: int = 1000,\n",
        "\t\tdevice: str = DEVICE,\n",
        "\t\tnum_timesteps: int = 30_000_000,\n",
        "\t\teval_frequency: int = 10,\n",
        "\t\tunroll_length: int = 5,\n",
        "\t\t# batch_size: int = 1024,\n",
        "\t\tnum_minibatches: int = 32,\n",
        "\t\tnum_update_epochs: int = 4,\n",
        "\t\treward_scaling: float = .1,\n",
        "\t\tentropy_cost: float = 1e-2,\n",
        "\t\tdiscounting: float = .97,\n",
        "\t\tlearning_rate: float = 3e-4,\n",
        "\t\tprogress_fn: Optional[Callable[[int, Dict[str, Any]], None]] = None,\n",
        "\t):\n",
        "\t\"\"\"\n",
        "\tTrains a policy via PPO.\n",
        "\t\"\"\"\n",
        "\t# gym_name = f'brax-{env_name}-v0'\n",
        "\tgym_name = f'FetchSlide-v2'\n",
        "\tif gym_name not in gym.envs.registry.keys():\n",
        "\t\tentry_point = functools.partial(envs.create_gym_env, env_name=env_name)\n",
        "\t\tgym.register(gym_name, entry_point=entry_point)\n",
        "\tenv = gym.make(gym_name, batch_size=num_envs, episode_length=episode_length)\n",
        "\t# automatically convert between jax ndarrays and torch tensors:\n",
        "\tenv = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "\n",
        "\t# env warmup\n",
        "\tenv.reset()\n",
        "\taction = torch.zeros(env.action_space.shape).to(device)\n",
        "\tenv.step(action)\n",
        "\n",
        "\t# create the agent\n",
        "\tpolicy_layers = [\n",
        "\t\t\tenv.observation_space.shape[-1], 64, 64, env.action_space.shape[-1] * 2\n",
        "\t]\n",
        "\tvalue_layers = [env.observation_space.shape[-1], 64, 64, 1]\n",
        "\tagent = PPOAgent(policy_layers, value_layers, entropy_cost, discounting,\n",
        "\t\t\t\t\t\t\t\treward_scaling, device)\n",
        "\tagent = torch.jit.script(agent.to(device))\n",
        "\toptimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "\tsps = 0\n",
        "\ttotal_steps = 0\n",
        "\ttotal_loss = 0\n",
        "\tfor eval_i in range(eval_frequency + 1):\n",
        "\t\tif progress_fn:\n",
        "\t\t\tt = time.time()\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tepisode_count, episode_reward = eval_unroll(agent, env, episode_length)\n",
        "\t\t\tduration = time.time() - t\n",
        "\t\t\t# TODO: only count stats from completed episodes\n",
        "\t\t\tepisode_avg_length = env.num_envs * episode_length / episode_count\n",
        "\t\t\teval_sps = env.num_envs * episode_length / duration\n",
        "\t\t\tprogress = {\n",
        "\t\t\t\t\t'eval/episode_reward': episode_reward,\n",
        "\t\t\t\t\t'eval/completed_episodes': episode_count,\n",
        "\t\t\t\t\t'eval/avg_episode_length': episode_avg_length,\n",
        "\t\t\t\t\t'speed/sps': sps,\n",
        "\t\t\t\t\t'speed/eval_sps': eval_sps,\n",
        "\t\t\t\t\t'losses/total_loss': total_loss,\n",
        "\t\t\t}\n",
        "\t\t\tprogress_fn(total_steps, progress)\n",
        "\n",
        "\t\tif eval_i == eval_frequency: break\n",
        "\n",
        "\t\tobservation = env.reset()\n",
        "\t\tnum_steps = batch_size * num_minibatches * unroll_length\n",
        "\t\tnum_epochs = num_timesteps // (num_steps * eval_frequency)\n",
        "\t\tnum_unrolls = batch_size * num_minibatches // env.num_envs\n",
        "\t\ttotal_loss = 0\n",
        "\t\tt = time.time()\n",
        "\t\tfor _ in range(num_epochs):\n",
        "\t\t\tobservation, td = train_unroll(agent, env, observation, num_unrolls, unroll_length)\n",
        "\n",
        "\t\t\t# make unroll first\n",
        "\t\t\tdef unroll_first(data):\n",
        "\t\t\t\tdata = data.swapaxes(0, 1)\n",
        "\t\t\t\treturn data.reshape([data.shape[0], -1] + list(data.shape[3:]))\n",
        "\t\t\ttd = sd_map(unroll_first, td)\n",
        "\n",
        "\t\t\t# update normalization statistics\n",
        "\t\t\tagent.update_normalization(td.observation)\n",
        "\n",
        "\t\t\tfor _ in range(num_update_epochs):\n",
        "\t\t\t\t# shuffle and batch the data\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tpermutation = torch.randperm(td.observation.shape[1], device=device)\n",
        "\t\t\t\t\tdef shuffle_batch(data):\n",
        "\t\t\t\t\t\tdata = data[:, permutation]\n",
        "\t\t\t\t\t\tdata = data.reshape([data.shape[0], num_minibatches, -1] + list(data.shape[2:]))\n",
        "\t\t\t\t\t\treturn data.swapaxes(0, 1)\n",
        "\t\t\t\t\tepoch_td = sd_map(shuffle_batch, td)\n",
        "\n",
        "\t\t\t\tfor minibatch_i in range(num_minibatches):\n",
        "\t\t\t\t\ttd_minibatch = sd_map(lambda d: d[minibatch_i], epoch_td)\n",
        "\t\t\t\t\tloss = agent.loss(td_minibatch._asdict())\n",
        "\t\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\t\tloss.backward()\n",
        "\t\t\t\t\toptimizer.step()\n",
        "\t\t\t\t\ttotal_loss += loss.detach()\n",
        "\n",
        "\t\tduration = time.time() - t\n",
        "\t\ttotal_steps += num_epochs * num_steps\n",
        "\t\ttotal_loss = total_loss / (num_epochs * num_update_epochs * num_minibatches)\n",
        "\t\tsps = num_epochs * num_steps / duration\n",
        "\t\n",
        "\treturn agent, env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2A9MMlHUajH"
      },
      "source": [
        "Let's go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "B-lrKHvkUeYM",
        "outputId": "447a410f-39dd-4f2e-aadf-927098226bcd"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "BaseRobotEnv.__init__() got an unexpected keyword argument 'batch_size'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     plt\u001b[39m.\u001b[39mplot(xdata, ydata)\n\u001b[1;32m     21\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> 23\u001b[0m agent, env \u001b[39m=\u001b[39m train(progress_fn\u001b[39m=\u001b[39;49mprogress)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtime to jit: \u001b[39m\u001b[39m{\u001b[39;00mtimes[\u001b[39m1\u001b[39m]\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mtimes[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtime to train: \u001b[39m\u001b[39m{\u001b[39;00mtimes[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mtimes[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[21], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env_name, num_envs, episode_length, device, num_timesteps, eval_frequency, unroll_length, batch_size, num_minibatches, num_update_epochs, reward_scaling, entropy_cost, discounting, learning_rate, progress_fn)\u001b[0m\n\u001b[1;32m     68\u001b[0m \tentry_point \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(envs\u001b[39m.\u001b[39mcreate_gym_env, env_name\u001b[39m=\u001b[39menv_name)\n\u001b[1;32m     69\u001b[0m \tgym\u001b[39m.\u001b[39mregister(gym_name, entry_point\u001b[39m=\u001b[39mentry_point)\n\u001b[0;32m---> 70\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(gym_name, batch_size\u001b[39m=\u001b[39;49mnum_envs, episode_length\u001b[39m=\u001b[39;49mepisode_length)\n\u001b[1;32m     71\u001b[0m \u001b[39m# automatically convert between jax ndarrays and torch tensors:\u001b[39;00m\n\u001b[1;32m     72\u001b[0m env \u001b[39m=\u001b[39m to_torch\u001b[39m.\u001b[39mJaxToTorchWrapper(env, device\u001b[39m=\u001b[39mdevice)\n",
            "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium/envs/registration.py:653\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n\u001b[1;32m    648\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou passed render_mode=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m\u001b[39m although \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt implement human-rendering natively. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGym tried to apply the HumanRendering wrapper but it looks like your environment is using the old \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrendering API, which is not supported by the HumanRendering wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    651\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 653\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    655\u001b[0m \u001b[39m# Copies the environment creation specification and kwargs to add to the environment specification details\u001b[39;00m\n\u001b[1;32m    656\u001b[0m spec_ \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(spec_)\n",
            "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium/envs/registration.py:641\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m     render_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs)\n\u001b[1;32m    642\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    643\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    644\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    646\u001b[0m     ):\n",
            "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium_robotics/envs/fetch/slide.py:163\u001b[0m, in \u001b[0;36mMujocoFetchSlideEnv.__init__\u001b[0;34m(self, reward_type, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, reward_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    157\u001b[0m     initial_qpos \u001b[39m=\u001b[39m {\n\u001b[1;32m    158\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrobot0:slide0\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.05\u001b[39m,\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrobot0:slide1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.48\u001b[39m,\n\u001b[1;32m    160\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrobot0:slide2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.0\u001b[39m,\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobject0:joint\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m1.7\u001b[39m, \u001b[39m1.1\u001b[39m, \u001b[39m0.41\u001b[39m, \u001b[39m1.0\u001b[39m, \u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m],\n\u001b[1;32m    162\u001b[0m     }\n\u001b[0;32m--> 163\u001b[0m     MujocoFetchEnv\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    165\u001b[0m         model_path\u001b[39m=\u001b[39;49mMODEL_XML_PATH,\n\u001b[1;32m    166\u001b[0m         has_object\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    167\u001b[0m         block_gripper\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    168\u001b[0m         n_substeps\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m    169\u001b[0m         gripper_extra_height\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m0.02\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m         target_in_the_air\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    171\u001b[0m         target_offset\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49marray([\u001b[39m0.4\u001b[39;49m, \u001b[39m0.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m]),\n\u001b[1;32m    172\u001b[0m         obj_range\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m         target_range\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m,\n\u001b[1;32m    174\u001b[0m         distance_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m,\n\u001b[1;32m    175\u001b[0m         initial_qpos\u001b[39m=\u001b[39;49minitial_qpos,\n\u001b[1;32m    176\u001b[0m         reward_type\u001b[39m=\u001b[39;49mreward_type,\n\u001b[1;32m    177\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    179\u001b[0m     EzPickle\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, reward_type\u001b[39m=\u001b[39mreward_type, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium_robotics/envs/fetch/fetch_env.py:293\u001b[0m, in \u001b[0;36mMujocoFetchEnv.__init__\u001b[0;34m(self, default_camera_config, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, default_camera_config: \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m DEFAULT_CAMERA_CONFIG, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 293\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(default_camera_config\u001b[39m=\u001b[39;49mdefault_camera_config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium_robotics/envs/fetch/fetch_env.py:69\u001b[0m, in \u001b[0;36mget_base_fetch_env.<locals>.BaseFetchEnv.__init__\u001b[0;34m(self, gripper_extra_height, block_gripper, has_object, target_in_the_air, target_offset, obj_range, target_range, distance_threshold, reward_type, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistance_threshold \u001b[39m=\u001b[39m distance_threshold\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_type \u001b[39m=\u001b[39m reward_type\n\u001b[0;32m---> 69\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(n_actions\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/reinforcement_learning/lib/python3.10/site-packages/gymnasium_robotics/envs/robot_env.py:280\u001b[0m, in \u001b[0;36mMujocoRobotEnv.__init__\u001b[0;34m(self, default_camera_config, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mujoco \u001b[39m=\u001b[39m mujoco\n\u001b[1;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_utils \u001b[39m=\u001b[39m mujoco_utils\n\u001b[0;32m--> 280\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    282\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco_rendering\u001b[39;00m \u001b[39mimport\u001b[39;00m MujocoRenderer\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmujoco_renderer \u001b[39m=\u001b[39m MujocoRenderer(\n\u001b[1;32m    285\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, default_camera_config\n\u001b[1;32m    286\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: BaseRobotEnv.__init__() got an unexpected keyword argument 'batch_size'"
          ]
        }
      ],
      "source": [
        "xdata = []\n",
        "ydata = []\n",
        "eval_sps = []\n",
        "train_sps = []\n",
        "times = [datetime.now()]\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "    times.append(datetime.now())\n",
        "    xdata.append(num_steps)\n",
        "    # copy to cpu, otherwise matplotlib throws an exception\n",
        "    reward = metrics['eval/episode_reward'].cpu()\n",
        "    ydata.append(reward)\n",
        "    eval_sps.append(metrics['speed/eval_sps'])\n",
        "    train_sps.append(metrics['speed/sps'])\n",
        "    clear_output(wait=True)\n",
        "    plt.xlim([0, 30_000_000])\n",
        "    plt.ylim([0, 6000])\n",
        "    plt.xlabel('# environment steps')\n",
        "    plt.ylabel('reward per episode')\n",
        "    plt.plot(xdata, ydata)\n",
        "    plt.show()\n",
        "\n",
        "agent, env = train(progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')\n",
        "print(f'eval steps/sec: {np.mean(eval_sps[1:])}')\n",
        "print(f'train steps/sec: {np.mean(train_sps[1:])}')\n",
        "# !nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2p-20bCi4iI"
      },
      "source": [
        "In this arrangement, we can rollout environment steps much faster than we can train: the speed at which PyTorch can backpropagate the loss and step the optimizer is the bottleneck.  This PyTorch code can probably be sped up by adding [automatic mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html), and following other recommendations in the [PyTorch performance tuning guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html).\n",
        "\n",
        "We know we have a fair bit of headroom to improve the PyTorch implementation, as the built-in Brax trainer (which uses [flax.optim](https://flax.readthedocs.io/en/latest/flax.optim.html)) runs at nearly double the steps per second:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmuz3I21p35H",
        "outputId": "a1b2fbb4-01dd-441a-ff73-cd9be69e0187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train steps/sec: 426827.7156297093\n"
          ]
        }
      ],
      "source": [
        "train_sps = []\n",
        "\n",
        "def progress(_, metrics):\n",
        "  if 'training/sps' in metrics:\n",
        "    train_sps.append(metrics['training/sps'])\n",
        "\n",
        "ppo.train(\n",
        "    environment=envs.get_environment(env_name='ant'), num_timesteps = 30_000_000,\n",
        "    num_evals = 10, reward_scaling = .1, episode_length = 1000,\n",
        "    normalize_observations = True, action_repeat = 1, unroll_length = 5,\n",
        "    num_minibatches = 32, num_updates_per_batch = 4, discounting = 0.97,\n",
        "    learning_rate = 3e-4, entropy_cost = 1e-2, num_envs = 2048,\n",
        "    batch_size = 1024, progress_fn = progress)\n",
        "\n",
        "print(f'train steps/sec: {np.mean(train_sps[1:])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqXKdDwVL6L4"
      },
      "source": [
        "tunaalabagana! ðŸ‘‹"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Brax Training with PyTorch on GPU",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "reinforcement_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b329387e251b95764b8f65684563519503b45dc8027da482b0a7bdbaa4a30d3e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
