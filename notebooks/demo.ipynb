{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCOanHc8JH_"
      },
      "source": [
        "## Demo for Grasp: Pick-and-Place with a robotic hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sOmCoOrF0F8"
      },
      "outputs": [],
      "source": [
        "#@title Install Brax and some helper modules\n",
        "#@markdown ## ⚠️ PLEASE NOTE:\n",
        "#@markdown This colab runs best using a TPU runtime.  From the Colab menu, choose Runtime > Change Runtime Type, then select **'TPU'** in the dropdown.\n",
        "\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import os\n",
        "from os import getcwd\n",
        "from os.path import join\n",
        "from IPython.display import HTML, Image, clear_output\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "from brax import envs\n",
        "from brax import jumpy as jp\n",
        "from brax.io import html\n",
        "from brax.io import image\n",
        "from brax.io import model\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.es import train as es\n",
        "from brax.training.agents.ars import train as ars\n",
        "import torch\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm8zbPBcJ5RJ"
      },
      "source": [
        "#### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "NaJDZqhCLovU",
        "outputId": "50994b20-d788-4264-af00-a3f06d58f943"
      },
      "outputs": [],
      "source": [
        "# Set up some constants\n",
        "SEED = 0\n",
        "ENV_NAME = \"grasp\"\n",
        "VIS_ALGO = \"ppo\" # \"ppo\", \"es\", \"ars\"\n",
        "VIS_STEPS = int(1e3) # how often to render the environment\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \n",
        "\n",
        "# Create a Brax environment\n",
        "env_name = ENV_NAME\n",
        "env = envs.get_environment(env_name=env_name)\n",
        "state = env.reset(rng=jp.random_prngkey(seed=SEED))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Details about the environment\n",
        "\n",
        "Source code for the env can be found [here](https://github.com/google/brax/blob/198dee3ac4/brax/envs/grasp.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HTML(html.render(env.sys, [state.qp]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initial state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.sys.num_actuators\n",
        "env.sys.num_bodies\n",
        "env.sys.num_joints\n",
        "env.sys.num_joint_dof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rollout = []\n",
        "rng = jax.random.PRNGKey(seed=SEED)\n",
        "state = env.reset(rng=rng)\n",
        "for _ in range(VIS_STEPS):\n",
        "  action = torch.rand((env.action_size,), device='cuda') * 2 - 1\n",
        "  state = env.step(state, action)\n",
        "  rollout.append(state)\n",
        "HTML(html.render(env.sys, [s.qp for s in rollout]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objectives\n",
        "\n",
        "1. Grasp trains an agent to pick up an object. Grasp observes three bodies: `Hand`, `Object`, and `Target`. When `Object` reaches `Target`, the agent is rewarded.\n",
        "2. The `reward function` is determined by the following factors: moving towards the object, being close to the object, touching the object, hitting the target and moving towards the target.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "reward &= moving\\ to\\ object + close\\ to\\ object + touching\\ object + 5 * target\\ hit + moving\\ to\\ target \\nonumber \\\\\n",
        "\\text{}  &\\nonumber \\\\\n",
        "\n",
        "reward &: \\text{final reward achieved.}  \\nonumber \\\\\n",
        "moving\\ to\\ object &: \\text{small reward for moving towards the object.} \\nonumber \\\\\n",
        "close\\ to\\ object &: \\text{small reward for being close to the object.} \\nonumber \\\\\n",
        "touching\\ object &: \\text{small reward for touching the object.} \\nonumber \\\\\n",
        "target\\ hit &: \\text{high reward for hitting the target (max. reward).} \\nonumber \\\\\n",
        "moving\\ to\\ target &: \\text{high reward for moving towards the target.} \\nonumber\n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "25iky4mITUhh"
      },
      "source": [
        "### Training the agent\n",
        "\n",
        "We will used the following training algorithms to train our RL agent:\n",
        "\n",
        "1. `Proximal policy optimization (PPO)`\n",
        "2. `Evolution Strategy (ES)`\n",
        "3. `Augmented Random Search (ARS)`\n",
        "\n",
        "#\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Proximal policy optimization (PPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_train_fn = functools.partial(ppo.train, num_timesteps=600_000_000, num_evals=10, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=20, num_minibatches=32, num_updates_per_batch=2, discounting=0.99, learning_rate=3e-4, entropy_cost=0.001, num_envs=2048, batch_size=256)\n",
        "\n",
        "max_y = 100\n",
        "min_y = 0\n",
        "\n",
        "ppo_xdata, ppo_ydata = [], []\n",
        "ppo_times = [datetime.now()]\n",
        "\n",
        "def ppo_progress(num_steps, metrics):\n",
        "  ppo_times.append(datetime.now())\n",
        "  ppo_xdata.append(num_steps)\n",
        "  ppo_ydata.append(metrics['eval/episode_reward'])\n",
        "  clear_output(wait=True)\n",
        "  plt.xlim([0, ppo_train_fn.keywords['num_timesteps']])\n",
        "  plt.ylim([min_y, max_y])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.plot(ppo_xdata, ppo_ydata)\n",
        "  plt.show()\n",
        "\n",
        "ppo_make_inference_fn, ppo_params, _ = ppo_train_fn(environment=env, progress_fn=ppo_progress)\n",
        "\n",
        "print(f'for \"PPO\": time to jit: {ppo_times[1] - ppo_times[0]}')\n",
        "print(f'for \"PPO\": time to train: {ppo_times[-1] - ppo_times[1]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Evolution Strategy (ES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "es_train_fn = functools.partial(es.train, num_timesteps=600_000_000, num_evals=10, episode_length=1000, normalize_observations=True, action_repeat=1, learning_rate=3e-4, population_size=1024)\n",
        "\n",
        "max_y = 100\n",
        "min_y = 0\n",
        "\n",
        "es_xdata, es_ydata = [], []\n",
        "es_times = [datetime.now()]\n",
        "\n",
        "def es_progress(num_steps, metrics):\n",
        "  es_times.append(datetime.now())\n",
        "  es_xdata.append(num_steps)\n",
        "  es_ydata.append(metrics['eval/episode_reward'])\n",
        "  clear_output(wait=True)\n",
        "  plt.xlim([0, es_train_fn.keywords['num_timesteps']])\n",
        "  plt.ylim([min_y, max_y])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.plot(es_xdata, es_ydata)\n",
        "  plt.show()\n",
        "\n",
        "es_make_inference_fn, es_params, _ = es_train_fn(environment=env, progress_fn=es_progress)\n",
        "\n",
        "print(f'for \"ES\": time to jit: {es_times[1] - es_times[0]}')\n",
        "print(f'for \"ES\": time to train: {es_times[-1] - es_times[1]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Augmented Random Search (ARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ars_train_fn = functools.partial(ars.train, num_timesteps=600_000_000, num_evals=10, episode_length=1000, normalize_observations=True, action_repeat=1, number_of_directions=1024)\n",
        "\n",
        "max_y = 100\n",
        "min_y = 0\n",
        "\n",
        "ars_xdata, ars_ydata = [], []\n",
        "ars_times = [datetime.now()]\n",
        "\n",
        "def ars_progress(num_steps, metrics):\n",
        "  ars_times.append(datetime.now())\n",
        "  ars_xdata.append(num_steps)\n",
        "  ars_ydata.append(metrics['eval/episode_reward'])\n",
        "  clear_output(wait=True)\n",
        "  plt.xlim([0, ars_train_fn.keywords['num_timesteps']])\n",
        "  plt.ylim([min_y, max_y])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.plot(ars_xdata, ars_ydata)\n",
        "  plt.show()\n",
        "\n",
        "ars_make_inference_fn, ars_params, _ = ars_train_fn(environment=env, progress_fn=ars_progress)\n",
        "\n",
        "print(f'for \"ARS\": time to jit: {ars_times[1] - ars_times[0]}')\n",
        "print(f'for \"ARS\": time to train: {ars_times[-1] - ars_times[1]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uUs9hbUKIH71"
      },
      "source": [
        "### Visualize the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "RNMLEyaTspEM",
        "outputId": "c03b0854-32a9-4286-9c3f-26f2f158e617"
      },
      "outputs": [],
      "source": [
        "params = ppo_params if VIS_ALGO == \"ppo\" else es_params if VIS_ALGO == \"es\" else ars_params\n",
        "make_inference_fn = ppo_make_inference_fn if VIS_ALGO == \"ppo\" else es_make_inference_fn if VIS_ALGO == \"es\" else ars_make_inference_fn\n",
        "inference_fn = make_inference_fn(params)\n",
        "\n",
        "env = envs.create(env_name=env_name)\n",
        "jit_env_reset = jax.jit(env.reset)\n",
        "jit_env_step = jax.jit(env.step)\n",
        "jit_inference_fn = jax.jit(inference_fn)\n",
        "\n",
        "rollout = []\n",
        "rng = jax.random.PRNGKey(seed=0)\n",
        "state = jit_env_reset(rng=rng)\n",
        "for _ in range(1000):\n",
        "  rollout.append(state)\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "  act, _ = jit_inference_fn(state.obs, act_rng)\n",
        "  state = jit_env_step(state, act)\n",
        "\n",
        "HTML(html.render(env.sys, [s.qp for s in rollout]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Brax Training.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "reinforcement_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b329387e251b95764b8f65684563519503b45dc8027da482b0a7bdbaa4a30d3e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
